{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essential libraries to start working with !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Libraries that we might need in the future !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# import keras\n",
    "\n",
    "# from keras.layers import Dense, Activation, Conv1D, MaxPooling1D, Flatten, LSTM, Dropout, GlobalMaxPooling1D\n",
    "# from tensorflow.keras.optimizers import Adam, SGD\n",
    "# from tensorflow.keras.models import Model\n",
    "# from tensorflow.keras import Sequential\n",
    "\n",
    "# from keras import regularizers\n",
    "# from sklearn import preprocessing\n",
    "# from sklearn.metrics import f1_score\n",
    "# from scipy.stats import kurtosis, skew\n",
    "# from sklearn.metrics import recall_score\n",
    "# from sklearn.metrics import roc_auc_score\n",
    "# from sklearn.datasets import make_circles\n",
    "# from sklearn.metrics import accuracy_score\n",
    "# from sklearn.metrics import precision_score\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "# from sklearn.metrics import cohen_kappa_score\n",
    "# from tensorflow.keras import utils as np_utils\n",
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "# from keras.utils import to_categorical\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import precision_recall_fscore_support\n",
    "# from sklearn.preprocessing import StandardScaler, MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading and filtering the original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_df = pd.read_csv('../data/ppg.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = original_df.copy()\n",
    "df['CREATE_DATETIME'] = pd.to_datetime(df['CREATE_DATETIME'])   # Transform to datetime\n",
    "\n",
    "df = df[df[\"READING_CATEGORY\"] == \"PPG\"]                        # Keeping only the PPG signals \n",
    "\n",
    "encoder = LabelEncoder()\n",
    "df['PATIENT_CODE'] = encoder.fit_transform(df['PATIENT_CODE'])  # Label encoding the IDs\n",
    "\n",
    "df.drop(columns=[\"ID\",\"READING_VALUE\",\"READING_CATEGORY\"],inplace=True)\n",
    "df.rename(columns={\"CREATE_DATETIME\":\"DATE\"},inplace=True)\n",
    "\n",
    "print(\"Nuber of unique patients:\",len(df[\"PATIENT_CODE\"].unique()))\n",
    "print(\"Unique years of birth:\",df[\"YEAR_OF_BIRTH\"].unique())\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = df[df[\"PATIENT_CODE\"] == 11]\n",
    "temp_df.reset_index(drop=True,inplace=True)\n",
    "temp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different methods for filtering the Singal\n",
    "\n",
    "TO-DO: Let's find more methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import signal\n",
    "\n",
    "def apply_lowpass_filter(ppg_signal, filter_type='butter', sampling_rate=50, cutoff=5):\n",
    "    nyquist_freq = 0.5 * sampling_rate\n",
    "    normalized_cutoff = cutoff / nyquist_freq\n",
    "    \n",
    "    if filter_type == 'butter':\n",
    "        b, a = signal.butter(4, normalized_cutoff, btype='low', analog=False)\n",
    "    elif filter_type == 'cheby1':\n",
    "        b, a = signal.cheby1(4, 0.5, normalized_cutoff, btype='low', analog=False)\n",
    "    elif filter_type == 'cheby2':\n",
    "        b, a = signal.cheby2(4, 20, normalized_cutoff, btype='low', analog=False)\n",
    "    elif filter_type == 'elliptic':\n",
    "        b, a = signal.ellip(4, 0.5, 20, normalized_cutoff, btype='low', analog=False)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid filter type. Choose 'butter', 'cheby1', 'cheby2', or 'elliptic'.\")\n",
    "    \n",
    "    ppg_filtered = signal.filtfilt(b, a, ppg_signal)\n",
    "    return ppg_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature extraction functions\n",
    "\n",
    "TO-DO: We have to enrich our features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.special\n",
    "from scipy.signal import find_peaks, welch\n",
    "from collections import Counter\n",
    "from pywt import wavedec\n",
    "\n",
    "def calculate_entropy(list_values):\n",
    "    counter_values = Counter(list_values).most_common()\n",
    "    probabilities = [elem[1]/len(list_values) for elem in counter_values]\n",
    "    entropy=scipy.stats.entropy(probabilities)\n",
    "    return entropy\n",
    "\n",
    "def calculate_statistics(list_values):\n",
    "    n5 = np.nanpercentile(list_values, 5)\n",
    "    n25 = np.nanpercentile(list_values, 25)\n",
    "    n75 = np.nanpercentile(list_values, 75)\n",
    "    n95 = np.nanpercentile(list_values, 95)\n",
    "    median = np.nanpercentile(list_values, 50)\n",
    "    mean = np.nanmean(list_values)\n",
    "    std = np.nanstd(list_values)\n",
    "    var = np.nanvar(list_values)\n",
    "    rms = np.nanmean(np.sqrt(list_values**2))\n",
    "    skew = pd.Series(list_values).skew()\n",
    "    kurtosis = pd.Series(list_values).kurt()\n",
    "    min = np.min(list_values)\n",
    "    max = np.max(list_values)\n",
    "\n",
    "    # Heart rate related features\n",
    "    # sampling_rate = 50\n",
    "    # peaks, _ = find_peaks(ppg_signal, height=0)                         # Find peaks in PPG signal\n",
    "    # instant_hr = 60 * len(peaks) / len(ppg_signal) * sampling_rate      # Instantaneous Heart Rate\n",
    "    # rri = np.diff(peaks) / sampling_rate                                # RR intervals\n",
    "    # hrv = np.std(rri)                                                   # Heart Rate Variability\n",
    "\n",
    "    # # Morphological features\n",
    "    # peaks, _ = find_peaks(ppg_signal, distance=50)\n",
    "    # peak_count = len(peaks)\n",
    "    # peak_mean = np.mean(ppg_signal[peaks]) if len(peaks) > 0 else 0\n",
    "    # peak_std = np.std(ppg_signal[peaks]) if len(peaks) > 0 else 0\n",
    "\n",
    "    # Frequency domain features\n",
    "    # freqs, psd = welch(ppg_signal)\n",
    "    # psd_mean = np.mean(psd)\n",
    "    # psd_std = np.std(psd)\n",
    "    # psd_peak = freqs[np.argmax(psd)]\n",
    "\n",
    "    # Time domain features\n",
    "    # diff_signal = np.diff(ppg_signal)\n",
    "    # diff_mean = np.mean(diff_signal)\n",
    "    # diff_std = np.std(diff_signal)\n",
    "\n",
    "    return [n5, n25, n75, n95, median, mean, std, var, rms, skew, kurtosis, min, max]\n",
    "\n",
    "def calculate_crossings(list_values):\n",
    "    zero_crossing_indices = np.nonzero(np.diff(np.array(list_values) > 0))[0]\n",
    "    no_zero_crossings = len(zero_crossing_indices)\n",
    "    mean_crossing_indices = np.nonzero(np.diff(np.array(list_values) > np.nanmean(list_values)))[0]\n",
    "    no_mean_crossings = len(mean_crossing_indices)\n",
    "    return [no_zero_crossings, no_mean_crossings]\n",
    "\n",
    "def get_features(list_values):\n",
    "    entropy = calculate_entropy(list_values)\n",
    "    crossings = calculate_crossings(list_values)\n",
    "    statistics = calculate_statistics(list_values)\n",
    "    return [entropy] + crossings + statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function that handles a ppg singal end-to-end (filtering --> analysis --> feature extraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(ppg_signal):\n",
    "    \n",
    "    # 1) Filtering the Signal\n",
    "    ppg_signal_filtered = apply_lowpass_filter(ppg_signal, filter_type='butter', sampling_rate=50, cutoff=5)\n",
    "\n",
    "    # 2) Doing the Wavelet analysis to extract the detailed and approximate coefficients (this is considered a filtering method as well)\n",
    "    coeffs = wavedec(ppg_signal_filtered, 'haar', level=5)\n",
    "    cA1, cD1, cD2, cD3, cD4, cD5 = coeffs\n",
    "\n",
    "    # Alternative: Compute the STFT of the signal\n",
    "    # # Set parameters for STFT\n",
    "    # fs = 50  # Sampling frequency (can be adjusted based on your data)\n",
    "    # nperseg = 5  # Length of each segment\n",
    "\n",
    "    # # Compute the STFT\n",
    "    # frequencies, times, Zxx = stft(time_series, fs=fs, nperseg=nperseg)\n",
    "\n",
    "    # # Step 3: Process the STFT output\n",
    "    # # Get the magnitude (absolute value) of the complex numbers\n",
    "    # magnitude_spectrum = np.abs(Zxx)\n",
    "\n",
    "    # 3) Extracting features from the detailed and approximate coefficients\n",
    "    features_1 = get_features(cD5)\n",
    "    features_2 = get_features(cD4)\n",
    "    features_3 = get_features(cD3)\n",
    "    features_4 = get_features(cD2)\n",
    "    features_5 = get_features(cD1)\n",
    "    features_6 = get_features(cA1)\n",
    "    \n",
    "    # 4) Combining all the features\n",
    "    features = features_1+features_2+features_3+features_4+features_5+features_6\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = []\n",
    "valid = []\n",
    "ids = []\n",
    "\n",
    "unique_patients = df[\"PATIENT_CODE\"].unique()\n",
    "\n",
    "unique_patients = [7,17]\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "for unique_patient in unique_patients:\n",
    "    temp_df = df[df[\"PATIENT_CODE\"] == unique_patient]\n",
    "    temp_df.reset_index(drop=True,inplace=True)\n",
    "\n",
    "    # Calculate the difference between consecutive dates\n",
    "    diff = temp_df['DATE'].diff()\n",
    "\n",
    "    # Find continuous dates\n",
    "    continuous_dates = diff.dt.total_seconds().fillna(0) == 1\n",
    "\n",
    "    # Split DataFrame into subdataframes with continuous dates\n",
    "    subdataframes = []\n",
    "    for i, continuous in enumerate(continuous_dates):\n",
    "        if i == 0 or not continuous:\n",
    "            subdataframes.append(temp_df.iloc[i:i+1])\n",
    "        else:\n",
    "            subdataframes[-1] = pd.concat([subdataframes[-1], temp_df.iloc[i:i+1]])\n",
    "\n",
    "    j=0\n",
    "    total_timeseries = 0\n",
    "\n",
    "    for i, subdf in enumerate(subdataframes):\n",
    "        if subdf.shape[0] >= 10:                        # Keeping only timeseries of at least 10\" of duration\n",
    "            subdf.reset_index(drop=True,inplace=True)\n",
    "\n",
    "            ppg_values = subdf[\"PPG_ARRAY\"].values\n",
    "            date_values = subdf[\"DATE\"].values\n",
    "\n",
    "            # duration = len(date_values)\n",
    "            # print(\"Duration in seconds: \", duration)\n",
    "\n",
    "            time_series = []\n",
    "            for seconds_values in ppg_values:\n",
    "                split_data = seconds_values[1:-1].split(', ')\n",
    "                integer_list = [int(item) for item in split_data]\n",
    "\n",
    "                for value in integer_list:\n",
    "                    time_series.append(value)\n",
    "\n",
    "            features = extract_features(time_series)\n",
    "\n",
    "            if subdf[\"PATIENT_CODE\"].values[0] == 7:\n",
    "                label = 0\n",
    "            else:\n",
    "                label = 1\n",
    "\n",
    "            X.append(features)\n",
    "            y.append(label) \n",
    "\n",
    "            j+=1\n",
    "        total_timeseries+=1\n",
    "\n",
    "    ids.append(temp_df[\"PATIENT_CODE\"].values[0])\n",
    "    total.append(total_timeseries)\n",
    "    valid.append(j)\n",
    "\n",
    "X = np.array(X)  # Transforming the list of feautures --> numpy arrays\n",
    "y = np.array(y)  # Transforming the list of labels--> numpy arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the selected patients and projecting the total number of CONTINUOUS timeseries and the timeseries above the predifiend time interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,10))\n",
    "\n",
    "bar_width = 0.3\n",
    "\n",
    "r1 = range(len(ids))\n",
    "r2 = [x + bar_width for x in r1]\n",
    "\n",
    "# Plotting\n",
    "plt.bar(r1, total, color='r', width=bar_width, edgecolor='grey', label='Total Timeseries')\n",
    "plt.bar(r2, valid, color='b', width=bar_width, edgecolor='grey', label='Timeseries > 10\"')\n",
    "\n",
    "# Add xticks on the middle of the group bars\n",
    "plt.xlabel('ID', fontweight='bold')\n",
    "plt.xticks([r + bar_width/2 for r in range(len(ids))], ids)\n",
    "\n",
    "plt.ylabel('Num of Timeseries', fontweight='bold')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Machine-Learning libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier, Perceptron, PassiveAggressiveClassifier, SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier, GradientBoostingClassifier, VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, GridSearchCV\n",
    "\n",
    "def find_best_model(X, y):\n",
    "    # Step 1: Normalize the data\n",
    "    scaler = MinMaxScaler()          \n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Aditional Step 1: Split the data into training and testing sets\n",
    "    # X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.1, shuffle=True, random_state=42)\n",
    "\n",
    "    models = {\n",
    "        'SVC': SVC(),\n",
    "        'NuSVC': NuSVC(),\n",
    "        'Linear SVC': LinearSVC(),\n",
    "        'Decision Tree': DecisionTreeClassifier(),\n",
    "        'MLP Classifier': MLPClassifier(),\n",
    "        'Gaussian NB': GaussianNB(),\n",
    "        'Bernoulli NB': BernoulliNB(),\n",
    "        'K-Neighbors': KNeighborsClassifier(),\n",
    "        'Linear Discriminant Analysis': LinearDiscriminantAnalysis(),\n",
    "        'Quadratic Discriminant Analysis': QuadraticDiscriminantAnalysis(),\n",
    "        'Perceptron': Perceptron(),\n",
    "        'SGD Classifier': SGDClassifier(),\n",
    "        'Ridge Classifier': RidgeClassifier(),\n",
    "        'Logistic Regression': LogisticRegression(),\n",
    "        'Passive Aggressive': PassiveAggressiveClassifier(),\n",
    "        'AdaBoost': AdaBoostClassifier(),\n",
    "        'Bagging': BaggingClassifier(),\n",
    "        'Voting': VotingClassifier(estimators=[\n",
    "            ('rf', RandomForestClassifier()),\n",
    "            ('dt', DecisionTreeClassifier())\n",
    "        ], voting='hard'),\n",
    "        'Random Forest': RandomForestClassifier(),\n",
    "        'Gradient Boosting': GradientBoostingClassifier(),\n",
    "        'Extra Trees': ExtraTreesClassifier(),\n",
    "        'XGBoost': xgb.XGBClassifier(n_estimators=100, use_label_encoder=False, objective='binary:logistic')\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    skf = StratifiedKFold(n_splits=10)\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        scores = cross_val_score(model, X_scaled, y, cv=skf, scoring='accuracy')\n",
    "        results[name] = scores.mean()\n",
    "    \n",
    "    # Step 2: Print the results in a presentable manner\n",
    "    results_df = pd.DataFrame.from_dict(results, orient='index', columns=['Accuracy'])\n",
    "    results_df = results_df.sort_values(by='Accuracy', ascending=False)\n",
    "    print(\"Cross-Validation Results (5-Fold):\")\n",
    "    print(results_df)\n",
    "    \n",
    "    # Aditional Step 2: Test the model's accuracy on the test set\n",
    "    # best_model_name = results_df.idxmax().values[0]\n",
    "    # best_model = models[best_model_name]\n",
    "    # best_model.fit(X_train, y_train)\n",
    "\n",
    "    # y_test_pred = best_model.predict(X_test)\n",
    "    # test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "    # print(f\"\\nBest Model: {best_model_name}\")\n",
    "    # print(f\"Test Set Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "    # return(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this if you run the alternative steps in \"find_best_model\" function: real_values , predictions = find_best_model(X,y)\n",
    "\n",
    "# confusion_matrix = metrics.confusion_matrix(real_values, predictions)\n",
    "\n",
    "# cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [0, 1])\n",
    "\n",
    "# cm_display.plot()\n",
    "# plt.show()\n",
    "\n",
    "# # Calculate precision, recall, F1 score, and accuracy\n",
    "# precision = metrics.precision_score(real_values, predictions)\n",
    "# recall = metrics.recall_score(real_values, predictions)\n",
    "# f1 = metrics.f1_score(real_values, predictions)\n",
    "# accuracy = metrics.accuracy_score(real_values, predictions)\n",
    "\n",
    "# # Print the results\n",
    "# print(f'Precision: {precision:.2f}')\n",
    "# print(f'Recall: {recall:.2f}')\n",
    "# print(f'F1 Score: {f1:.2f}')\n",
    "# print(f'Accuracy: {accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_best_model(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost - Finding best parameters/model \n",
    "\n",
    "We might this later - IGNORE for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import xgboost as xgb\n",
    "# from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "\n",
    "# def find_best_xgboost_model(X, y):\n",
    "#     # Step 1 & 2: Scale the data\n",
    "#     scaler = MinMaxScaler()          \n",
    "#     X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "#     # Step 2: Split the data into training and testing sets\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, shuffle=True, random_state=42)\n",
    "   \n",
    "#     # Step 3: Define the parameter grid for XGBoost\n",
    "#     # param_grid = {\n",
    "#     #     'n_estimators': [100, 200, 300, 400, 500],\n",
    "#     #     'max_depth': [3, 4, 5, 6, 7, 8],\n",
    "#     #     'learning_rate': [0.01, 0.05, 0.1, 0.15, 0.2],\n",
    "#     #     'subsample': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "#     #     'colsample_bytree': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "#     #     'gamma': [0, 0.1, 0.2, 0.3],\n",
    "#     #     'min_child_weight': [1, 2, 3, 4, 5]\n",
    "#     # }\n",
    "\n",
    "#     param_grid = {\n",
    "#         'n_estimators': [100, 200, 300],\n",
    "#         'max_depth': [3, 4, 5, 6],\n",
    "#         'learning_rate': [0.01, 0.05, 0.1],\n",
    "#         'subsample': [0.6, 0.8, 1.0],\n",
    "#         'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "#         'gamma': [0, 0.1, 0.2],\n",
    "#     }\n",
    "    \n",
    "#     # Step 4: Set up the XGBoost classifier\n",
    "#     xgb_clf = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "    \n",
    "#     # Step 5: Set up the cross-validation and grid search\n",
    "#     skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "#     grid_search = GridSearchCV(estimator=xgb_clf, param_grid=param_grid, scoring='accuracy', cv=skf, verbose=2, n_jobs=-1)\n",
    "    \n",
    "#     # Step 6: Perform the grid search\n",
    "#     grid_search.fit(X_train, y_train)\n",
    "    \n",
    "#     # Step 7: Get the best model and parameters\n",
    "#     best_model = grid_search.best_estimator_\n",
    "#     best_params = grid_search.best_params_\n",
    "    \n",
    "#     # Step 8: Print the best parameters\n",
    "#     print(\"Best Parameters found: \", best_params)\n",
    "    \n",
    "#     # Step 9: Test the best model's accuracy on the test set\n",
    "#     y_test_pred = best_model.predict(X_test)\n",
    "#     test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "#     print(f\"Test Set Accuracy: {test_accuracy:.4f}\")\n",
    "    \n",
    "#     return best_model, best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model, params = find_best_xgboost_model(X, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
